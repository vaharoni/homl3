{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s nice introduction to this topic, then choose a particular embedded Reber grammar (such as the one represented on Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t.\n",
    "\n",
    "https://www.willamette.edu/~gorr/classes/cs449/reber.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorrt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = set('abc')\n",
    "x.add('d')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(list('BTSXPVE'))\n",
    "\n",
    "class ReberNode:\n",
    "    def __init__(self) -> None:\n",
    "        self.links = []\n",
    "\n",
    "    def add(self, letter: str, node = None):\n",
    "        if node is None:\n",
    "            node = ReberNode()\n",
    "        self.links.append(dict(letter=letter, node=node))\n",
    "        return node\n",
    "    \n",
    "    def get_invalid_letter(self):\n",
    "        invalid_letters = list(vocabulary - {x['letter'] for x in self.links})\n",
    "        return np.random.choice(invalid_letters)\n",
    "    \n",
    "    # We assume for simplicity that a given letter can only be sent to one node\n",
    "    def get_valid_node(self, letter):\n",
    "        for node in self.links:\n",
    "            if node['letter'] == letter:\n",
    "                return node['node']\n",
    "        return None\n",
    "\n",
    "    def generate(self, error_chance=0, result=None):\n",
    "        if result is None:\n",
    "            result = []\n",
    "\n",
    "        if len(self.links) == 0:\n",
    "            return ''.join(result)\n",
    "        \n",
    "        next = self.links[np.random.randint(len(self.links))]\n",
    "        if np.random.random() < error_chance:\n",
    "            result.append(self.get_invalid_letter())\n",
    "        else:\n",
    "            result.append(next['letter'])\n",
    "        return next['node'].generate(error_chance, result)\n",
    "    \n",
    "    def is_valid(self, text):\n",
    "        if text == '' and len(self.links) == 0:\n",
    "            return True\n",
    "        first_letter, *rest = list(text)\n",
    "        next_node = self.get_valid_node(first_letter)\n",
    "        if next_node is None:\n",
    "            return False\n",
    "        return next_node.is_valid(''.join(rest))\n",
    "    \n",
    "    def is_partial_valid(self, text, result=None):\n",
    "        if result is None:\n",
    "            result = []\n",
    "        if text == '' and len(self.links) == 0:\n",
    "            return result\n",
    "        first_letter, *rest = list(text)\n",
    "        next_node = self.get_valid_node(first_letter)\n",
    "        if next_node is None:\n",
    "            return result + [0] * len(text)\n",
    "        result.append(1)\n",
    "        return next_node.is_partial_valid(''.join(rest), result)\n",
    "\n",
    "def create_inner():\n",
    "    n0 = ReberNode()\n",
    "    n1 = n0.add('B')\n",
    "    n2 = n1.add('T')\n",
    "    n2.add('S', n2)\n",
    "    n3 = n2.add('X')\n",
    "    n4 = n3.add('S')\n",
    "\n",
    "    n5 = n1.add('P')\n",
    "    n5.add('T', n5)\n",
    "    n6 = n5.add('V')\n",
    "    n6.add('V', n4)\n",
    "\n",
    "    n3.add('X', n5)\n",
    "    n6.add('P', n3)\n",
    "\n",
    "    n_end = n4.add('E')\n",
    "    return (n0, n_end)\n",
    "\n",
    "def create_outer():\n",
    "    i1_start, i1_end = create_inner()\n",
    "    i2_start, i2_end = create_inner()\n",
    "\n",
    "    n0 = ReberNode()\n",
    "    n1 = n0.add('B')\n",
    "\n",
    "    n1.add('T', i1_start)\n",
    "    n2 = i1_end.add('T')\n",
    "\n",
    "    n1.add('P', i2_start)\n",
    "    i2_end.add('P', n2)\n",
    "\n",
    "    n_end = n2.add('E')\n",
    "    return n0\n",
    "\n",
    "def gen(grammar, error_rate=0):\n",
    "    string = grammar.generate(error_rate)\n",
    "    is_valid = grammar.is_valid(string)\n",
    "    return string, is_valid\n",
    "\n",
    "grammar = create_outer()\n",
    "\n",
    "grammar.is_partial_valid('BTIT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = set()\n",
    "invalid_set = set()\n",
    "target = 20_000\n",
    "\n",
    "for _ in range(target * 100):\n",
    "    text, valid = gen(grammar)\n",
    "    if not valid:\n",
    "        print('PROBLEM!')\n",
    "        break\n",
    "    if len(valid_set) < target:\n",
    "        valid_set.add(text)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "for _ in range(target * 100):\n",
    "    text, valid = gen(grammar, 0.1)\n",
    "    if not valid:\n",
    "        if len(invalid_set) < target:\n",
    "            invalid_set.add(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(len(valid_set))\n",
    "print(len(invalid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation: 2000\n",
    "# test: 2000\n",
    "# training: 12000\n",
    "valid_list = list(valid_set)\n",
    "invalid_list = list(invalid_set)\n",
    "\n",
    "training_ok = valid_list[:12000]\n",
    "valid_ok = valid_list[12000:14000]\n",
    "test_ok = valid_list[14000:]\n",
    "\n",
    "training_bad = invalid_list[:12000]\n",
    "valid_bad = invalid_list[12000:14000]\n",
    "test_bad = invalid_list[14000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.array([len(x) for x in valid_list])))\n",
    "print(np.max(np.array([len(x) for x in invalid_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(valid, invalid, shuffle=False, batch_size=32):\n",
    "    valid_ds = tf.data.Dataset.from_tensor_slices(tf.constant(valid, dtype=tf.string)).map(lambda x: (x, 1.0))\n",
    "    invalid_ds = tf.data.Dataset.from_tensor_slices(tf.constant(invalid, dtype=tf.string)).map(lambda x: (x, 0.0))\n",
    "    ds = valid_ds.concatenate(invalid_ds)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000)\n",
    "    return ds.cache().batch(batch_size).prefetch(1)\n",
    "\n",
    "training_ds = create_dataset(training_ok, training_bad, shuffle=True)\n",
    "valid_ds = create_dataset(valid_ok, valid_bad)\n",
    "test_ds = create_dataset(test_ok, test_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'BPBTSXXTTTTTTTTVPXVPSEPE' b'BTBPVPPTVVBTE'\n",
      " b'BTBPTTTTTVPXVPXTTTVPXVVETE' b'BTBTSSXXTTVPXTVPXVPXTTVPSETE'\n",
      " b'BEBTXXTTVVETE' b'BTBTSSXXXPXTVVETE' b'BPBPTTTTTTTTVPXTTTTTTVPSEPE'\n",
      " b'BPBTSSSSSSSXXVPXTTTTTVPSEPE' b'BEBPTVTSEPE' b'BPSTSBXVVEPE'], shape=(10,), dtype=string)\n",
      "tf.Tensor([1. 0. 1. 1. 0. 0. 1. 1. 0. 0.], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in training_ds.take(1):\n",
    "    print(x[:10])\n",
    "    print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2 5 ... 0 0 0]\n",
      " [5 7 5 ... 0 0 0]\n",
      " [5 2 5 ... 0 0 0]\n",
      " ...\n",
      " [5 7 5 ... 0 0 0]\n",
      " [5 7 5 ... 0 0 0]\n",
      " [5 7 5 ... 0 0 0]], shape=(32, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocab = [x.lower() for x in vocabulary]\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(vocabulary=vocab, split='character', max_tokens=60, pad_to_max_tokens=True)\n",
    "text_vec_layer.get_vocabulary()\n",
    "for x in training_ds.map(lambda x, y: x).take(1):\n",
    "    print(text_vec_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 32s 16ms/step - loss: 0.1855 - binary_accuracy: 0.9300 - val_loss: 0.1446 - val_binary_accuracy: 0.9503\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1217 - binary_accuracy: 0.9588 - val_loss: 0.1304 - val_binary_accuracy: 0.9590\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.1031 - binary_accuracy: 0.9651 - val_loss: 0.0992 - val_binary_accuracy: 0.9685\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.1022 - binary_accuracy: 0.9645 - val_loss: 0.1158 - val_binary_accuracy: 0.9638\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.0901 - binary_accuracy: 0.9694 - val_loss: 0.0988 - val_binary_accuracy: 0.9670\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.0805 - binary_accuracy: 0.9719 - val_loss: 0.0885 - val_binary_accuracy: 0.9707\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.0774 - binary_accuracy: 0.9732 - val_loss: 0.0828 - val_binary_accuracy: 0.9732\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.0714 - binary_accuracy: 0.9753 - val_loss: 0.0702 - val_binary_accuracy: 0.9785\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.0672 - binary_accuracy: 0.9768 - val_loss: 0.0660 - val_binary_accuracy: 0.9797\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.0712 - binary_accuracy: 0.9759 - val_loss: 0.0836 - val_binary_accuracy: 0.9720\n"
     ]
    }
   ],
   "source": [
    "vocab_size = text_vec_layer.vocabulary_size()\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, vocab_size, mask_zero=True),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "hist = model.fit(training_ds, epochs=10, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 19s 20ms/step - loss: 0.1926 - binary_accuracy: 0.9333 - val_loss: 0.1775 - val_binary_accuracy: 0.9388\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.1721 - binary_accuracy: 0.9421 - val_loss: 0.2035 - val_binary_accuracy: 0.9300\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.1673 - binary_accuracy: 0.9426 - val_loss: 0.1695 - val_binary_accuracy: 0.9427\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.1498 - binary_accuracy: 0.9487 - val_loss: 0.1405 - val_binary_accuracy: 0.9545\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.1251 - binary_accuracy: 0.9583 - val_loss: 0.1396 - val_binary_accuracy: 0.9530\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0896 - binary_accuracy: 0.9688 - val_loss: 0.0737 - val_binary_accuracy: 0.9735\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0728 - binary_accuracy: 0.9753 - val_loss: 0.0612 - val_binary_accuracy: 0.9780\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0648 - binary_accuracy: 0.9777 - val_loss: 0.0567 - val_binary_accuracy: 0.9795\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0508 - binary_accuracy: 0.9826 - val_loss: 0.0453 - val_binary_accuracy: 0.9858\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0425 - binary_accuracy: 0.9856 - val_loss: 0.0341 - val_binary_accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "vocab_size = text_vec_layer.vocabulary_size()\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, vocab_size, mask_zero=True),\n",
    "    tf.keras.layers.LSTM(512),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "hist = model.fit(training_ds, epochs=10, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36811\n"
     ]
    }
   ],
   "source": [
    "# validation: 2000\n",
    "# test: 2000\n",
    "# training: 12000\n",
    "full_list = list([(x, grammar.is_partial_valid(x)) for x in valid_set | invalid_set])\n",
    "np.random.shuffle(full_list)\n",
    "print(len(full_list))\n",
    "\n",
    "training_arr = full_list[:28_000]\n",
    "validation_arr = full_list[28_000:32_000]\n",
    "test_arr = full_list[32_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 39), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings, is_partial_valid = zip(*test_arr[100:200])\n",
    "strings = tf.constant(strings)\n",
    "labels = tf.ragged.constant(is_partial_valid)\n",
    "print(np.max([len(x) for x in is_partial_valid]))\n",
    "labels.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_2(array, shuffle=False, batch_size=32):\n",
    "    strings, is_partial_valid = zip(*array)\n",
    "    inputs = tf.constant(strings, dtype=tf.string)\n",
    "    labels = tf.ragged.constant(is_partial_valid, dtype=tf.float32).to_tensor()\n",
    "    ds = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000)\n",
    "    return ds.cache().batch(batch_size).map(lambda x, y: (x, y[:, :tf.reduce_max(tf.strings.length(x)), tf.newaxis])).prefetch(1)\n",
    "\n",
    "training_ds = create_dataset_2(training_arr, shuffle=True)\n",
    "valid_ds = create_dataset_2(validation_arr)\n",
    "test_ds = create_dataset_2(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'BTBPVPXTVPXTTTVPXVPXTVVETE' b'BPBTSSSSSSSSXXVPXVPXTVVEPE'\n",
      " b'BTBTSXXTTTVPXTTTTTVVETE' b'BTBPTTTVPXTTTTTTTVPXVVETE'\n",
      " b'BPBTSSSSXEVVEPE' b'BXBPVPVEPE' b'BTBTSSSXXTTTTVPXVPXVPXVVETE'\n",
      " b'BTBTXXVPSEEE' b'BTBTXXTTTVPXVPXVVETE' b'BTXPTTVPXTTTVPSETS'\n",
      " b'BPBPTSVEPS' b'BPETVSXPE' b'BPBPVPXVPXTVPXTVPXTVPXTVPXTVVEPE'\n",
      " b'BPBPVPXTTVPXTTVPXVPXTVVEPE' b'BPBPVPXVBSPS'\n",
      " b'BPBPTTVPXTTTVPXTTTVPXVVEPE' b'BPBPPPXVPSEPB' b'BSBPTVPSEPE'\n",
      " b'BVBPTVPETVPSETE' b'BTBPTTTTTTTTVPXVPSETE'\n",
      " b'BPBPTTVPXVPXTTTTVPXTVPXTTVVEPE' b'BTBPVPXTTPVPSETE'\n",
      " b'BTBPTTTVPXVPXTTVPXTTVPXVVETE' b'BPBTSSSXXVPXVPXTTVPXTTVPXTVPSEPE'\n",
      " b'BTBTSXXTTTTTTVPXVPXTTVVETE' b'BPBPTBTTTVPSEPE'\n",
      " b'BTBTXXTVPXTVPXTVPXVPXVPSETE' b'BTBPTTTTTTTVPXTVPXTTVPSETE' b'BVBPVVEPE'\n",
      " b'BPBTXXVPXVPXVPXVPXTTVVEPE' b'TTVTSBXXSVVETE' b'BPBPPTTVVEPB'], shape=(32,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]], shape=(32, 32, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[b'BTBTXXTVPSETX' b'BTBTSXXTTTTTTTVPXVVETE' b'BTBPTVPXVVEVE'\n",
      " b'BEBPXTTVVEPE' b'BTBPVPXTTTTVPXVPXTTVPXVPSETE' b'BTBPVPXVPSETE'\n",
      " b'TPBTXBSPE' b'BTBPTTTTVPXTTTTTTTTVPXVPSETE' b'BPBPTTTXTTTVPVTXVVEPE'\n",
      " b'BPBTSSPXXPSEPE' b'BTBTSSSSXXTTTVPXTTTVVETE' b'BPETSSSXXVVEPE'\n",
      " b'BPBTXXTVPXVPXTTVPXTTTVPSEPE' b'BTBTSXXTTTVPXTVPXVPXVPSETE'\n",
      " b'EPBPTSVVSPE' b'BBBTSSVSXSEPE' b'BPBTSXXTTTTTTTTTTTVPXVPXTVVEPE'\n",
      " b'BPBPTTTVPXTTTTTTVPSEPE' b'BEBPVPSEPS' b'BTBTSSSSSSSSSXXTVPXTVPXTVVETE'\n",
      " b'VPBSSXXTTTTBTTTTTEVEPE' b'BTBPTTTTVPXVPXVPXVVETE'\n",
      " b'BPBTSXXTTTTTTVPXVPXVPXVVEPE' b'BVBPTTTVPXTVPXBTVVEPE' b'BVBTXSEPV'\n",
      " b'BPBTVSSXXVPSEPE' b'BTBTSSXXTTTVVEEE' b'BPBTSXXVVEPT' b'BVBPSSETE'\n",
      " b'BTBTSSSXXVPXVPXTTVPXTVVETE' b'BTBTSXXTTTTTTTVPXTTTTVVETE'\n",
      " b'BTBSTTVPPVPSETE'], shape=(32,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]], shape=(32, 30, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_ds.take(2):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "875/875 [==============================] - 21s 20ms/step - loss: 0.3482 - binary_accuracy: 0.8756 - val_loss: 0.2736 - val_binary_accuracy: 0.9180\n",
      "Epoch 2/20\n",
      "875/875 [==============================] - 16s 18ms/step - loss: 0.2102 - binary_accuracy: 0.9400 - val_loss: 0.1635 - val_binary_accuracy: 0.9544\n",
      "Epoch 3/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.2810 - binary_accuracy: 0.9197 - val_loss: 0.2023 - val_binary_accuracy: 0.9366\n",
      "Epoch 4/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.1746 - binary_accuracy: 0.9461 - val_loss: 0.1510 - val_binary_accuracy: 0.9530\n",
      "Epoch 5/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.1378 - binary_accuracy: 0.9587 - val_loss: 0.1239 - val_binary_accuracy: 0.9624\n",
      "Epoch 6/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.1135 - binary_accuracy: 0.9661 - val_loss: 0.0966 - val_binary_accuracy: 0.9714\n",
      "Epoch 7/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0920 - binary_accuracy: 0.9729 - val_loss: 0.0806 - val_binary_accuracy: 0.9769\n",
      "Epoch 8/20\n",
      "875/875 [==============================] - 14s 16ms/step - loss: 0.0568 - binary_accuracy: 0.9827 - val_loss: 0.0413 - val_binary_accuracy: 0.9871\n",
      "Epoch 9/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.1288 - binary_accuracy: 0.9640 - val_loss: 0.1929 - val_binary_accuracy: 0.9437\n",
      "Epoch 10/20\n",
      "875/875 [==============================] - 16s 18ms/step - loss: 0.1324 - binary_accuracy: 0.9610 - val_loss: 0.0936 - val_binary_accuracy: 0.9724\n",
      "Epoch 11/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0673 - binary_accuracy: 0.9807 - val_loss: 0.0370 - val_binary_accuracy: 0.9895\n",
      "Epoch 12/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0686 - binary_accuracy: 0.9816 - val_loss: 0.0253 - val_binary_accuracy: 0.9930\n",
      "Epoch 13/20\n",
      "875/875 [==============================] - 14s 16ms/step - loss: 0.0206 - binary_accuracy: 0.9942 - val_loss: 0.0148 - val_binary_accuracy: 0.9962\n",
      "Epoch 14/20\n",
      "875/875 [==============================] - 14s 16ms/step - loss: 0.0339 - binary_accuracy: 0.9915 - val_loss: 0.0309 - val_binary_accuracy: 0.9923\n",
      "Epoch 15/20\n",
      "875/875 [==============================] - 15s 18ms/step - loss: 0.0143 - binary_accuracy: 0.9964 - val_loss: 0.0096 - val_binary_accuracy: 0.9979\n",
      "Epoch 16/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0083 - binary_accuracy: 0.9980 - val_loss: 0.0059 - val_binary_accuracy: 0.9985\n",
      "Epoch 17/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0053 - binary_accuracy: 0.9988 - val_loss: 0.0036 - val_binary_accuracy: 0.9993\n",
      "Epoch 18/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0034 - binary_accuracy: 0.9993 - val_loss: 0.0023 - val_binary_accuracy: 0.9997\n",
      "Epoch 19/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0024 - binary_accuracy: 0.9996 - val_loss: 0.0016 - val_binary_accuracy: 0.9998\n",
      "Epoch 20/20\n",
      "875/875 [==============================] - 15s 17ms/step - loss: 0.0018 - binary_accuracy: 0.9996 - val_loss: 0.0012 - val_binary_accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "vocab_size = text_vec_layer.vocabulary_size()\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, vocab_size, mask_zero=True),\n",
    "    # output ~ (32, sequence length, 128)\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    # output ~ (32, sequence length, 1)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "hist = model.fit(training_ds, epochs=20, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/151 [..............................] - ETA: 5s - loss: 7.7568e-04 - binary_accuracy: 1.0000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 2s 10ms/step - loss: 0.0014 - binary_accuracy: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0013762167654931545, 0.9997318983078003]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.43238285, 0.9999465 ], dtype=float32)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strings = tf.constant([\"BPXXTTVPXVPXTTTTTVVETE\", \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"])\n",
    "# test_strings\n",
    "\n",
    "y_proba = model.predict(test_strings)\n",
    "y_proba[:, -1, 0]\n",
    "# print()\n",
    "# print(\"Estimated probability that these are Reber strings:\")\n",
    "# for index, string in enumerate(test_strings):\n",
    "#     print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[1, 2, 3, 4, 5], [1, 2, 3]]>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ragged.constant([[1, 2, 3, 4, 5], [1,2,3]], ragged_rank=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
