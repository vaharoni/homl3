{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you define reinforcement learning? How is it different from regular supervised or unsupervised learning? \n",
    "\n",
    "> RL is a branch in ML where we provide an agent with an environment that gives inherent rewards (goal) and punishments (failure to achieve the goal, e.g. fallinig) to the agent based on its actions. Unlike supervised learning, we do not provide labeled data - the agent is left to their own device to experiment and figure out what they need to do. In a way, it is similar to unsupervised learning, but the parameters it uses in order to optimize the outcome are policy parameters about how to interact with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of three possible applications of RL that were not mentioned in this chapter? For each of them, what is the environment? What is the agent? What are some possible actions? What are the rewards?\n",
    "\n",
    "> Stock investing. Environment - past stock data. Actions - buy and sell operations. Rewards - daily output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the discount factor? Can the optimal policy change if you modify the discount factor?\n",
    "\n",
    "> This is the factor we use in order to discount future rewards. Yeah, if we modify discount factors, the rewards are changing, hence the optimal policy could change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you measure the performance of a reinforcement learning agent? \n",
    "\n",
    "> Based on the rewards it got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the credit assignment problem? When does it occur? How can you alleviate it? \n",
    "\n",
    "> How do we attribute rewards of an action, given that its ramifications can sometimes only be observed well into the future. We use the discount factor to alleviate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the point of using a replay buffer? \n",
    "\n",
    "> The point is to break the correlation in the sequential data. We can then just randomly sample states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an off-policy RL algorithm?\n",
    "\n",
    "> It's an algorithm that updates a policy while following a different policy (e.g. random behavior)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
