{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the main tasks that autoencoders are used for? \n",
    "\n",
    "> Autoencoders are a form of dimensionality reduction. They are useful for creatoing a compact representation of the input, forcing them to learn the important features. They can be used in a complete unsupervised fashion, as they are asked to generate the input that they are given. We can ensure our representation of the input works well before feeding it to other types of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed? \n",
    "\n",
    "> Autoencoders can be trained with no supervision, since they are simply trained on the input. So we can train our autoencoder (encoder + decoder pair) to make sure we get useful representations of the input, then copy the weights of the encoder part to the model and train it on the labeled instances we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?\n",
    "\n",
    "> Not necessarily. The point of the encoder is to be able to find useful representation of the data typically in lower dimensionality than the data itself. A naive pass-through encoder with the same dimensions as the input is not helpful at all. To evaluate the performance of an autoencoder, we look at the reconstruction loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder? \n",
    "\n",
    "> Undercomplete autoencoder is one that has a bottleneck layer to reduce dimensionality. An excessively undercomplete autoencoder may miss features that are important for our application. The risk of overcomplete autoencoder is that it will not be useful at all, e.g. just copy through the values from the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How do you tie weights in a stacked autoencoder? What is the point of doing so? \n",
    "\n",
    "> We can essentially create a layer that takes as input an input layer, and transposes its kernel (biases are still trained). This is useful in the decoder part, since it reduces (by up to half) the number of parameters the autoencoder needs to learn. This pushese the encoder part to learn representations that are close to reversible when transposed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is a generative model? Can you name a type of generative autoencoder? \n",
    "\n",
    "> A generative model is a model that generates data that look similar to our inputs. Variational autoencoder is a type of a generative model. It projects the input data into a probabilistic latent space that is regularized to be close to gaussian. Then we can simply provide gaussian noise and get outputs that look like our inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is a GAN? Can you name a few tasks where GANs can shine? \n",
    "\n",
    "> GAN is a Generative Adverserial Network. It has a generator (the decoder part of an autoencoder that starts with noise as its inputs) and a discriminator that has the goal of determining whether the input it receives is real or fake. Since its goal is to trick the discriminator, the generator learns to produce convincing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What are the main difficulties when training GANs? \n",
    "\n",
    "> They are notoriously unstable, and might never get to the nash equilibrium. The generator may learn to become really good at faking one particular class, forgetting other classes, until the discriminator starts determining it is fake, which pushes the generator to learn how to take another class forgetting the previous one, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What are diffusion models good at? What is their main limitation?\n",
    "\n",
    "> Similar to GANs, diffusion models allow generating new images from noise. Their main limitation is that they are slow as they need to go through many time steps to remove the gaussian noise until an image emerges."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
