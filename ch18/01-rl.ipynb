{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorrt\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gym.envs.registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "# horizontal position (0.0 means center)\n",
    "# velocity (positive means right)\n",
    "# angle of pole (0.0 means vertical)\n",
    "# angular velocity (positive means clockwise) \n",
    "print(obs)\n",
    "# Environment-specific. Here it is empty.\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 600, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "1.0\n",
      "False\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "action = 1 # accelerate right\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(truncated)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 600, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a hard-coded policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.698, 8.389445512070509, 24.0, 63.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_reward = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    totals.append(episode_reward)\n",
    "\n",
    "np.mean(totals), np.std(totals), min(totals), max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inner working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)  # True => right, False => left\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, dtype=tf.float32)\n",
    "        # The loss function is used here to increase the probability of the action that was just taken.\n",
    "        # If the the model predicted 0.2 (20% chance of going left) and the randomized action ended up \n",
    "        # with us going right, then int(action)=1 and y_target=0 (i.e. \"0% chance of going left\"), so we \n",
    "        # compute the loss between 0.2 and 0 so that next time the model is more confident in predicting\n",
    "        # a number closer to 0.\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) \n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-22 -40 -50]\n",
      "[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\n"
     ]
    }
   ],
   "source": [
    "print(discount_rewards([10, 0, -50], discount_factor=0.8))\n",
    "print(discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_iterations, n_episodes_per_update, n_max_steps, discount_factor):\n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "        all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "        all_mean_grads = []\n",
    "        for var_index in range(len(model.trainable_variables)):\n",
    "            # For each trainable variable, compute the weighted mean of its gradients over all episodes and all steps,\n",
    "            # weighted by the final_reward. Initially, the gradients per variable reflect a step towards the action\n",
    "            # actually taken. But as we introduce rewards, this can change: negative rewards will actually push \n",
    "            # it to the opposite direction of the gradient.\n",
    "            mean_grads = tf.reduce_mean(\n",
    "                [\n",
    "                    final_reward * all_grads[episode_index][step][var_index]\n",
    "                    for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                        for step, final_reward in enumerate(final_rewards)\n",
    "                ], axis=0)\n",
    "            all_mean_grads.append(mean_grads)\n",
    "        optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "        print('.', end='')\n",
    "\n",
    "def train_or_load_model(model, model_folder, **kwargs):\n",
    "    if Path(model_folder).exists():\n",
    "        return tf.keras.models.load_model(model_folder)\n",
    "    else:\n",
    "        train_model(model=model, **kwargs)\n",
    "        model.save(model_folder, save_format='tf')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = train_or_load_model(\n",
    "    model, \n",
    "    'data/01-policy/01',\n",
    "    n_iterations=n_iterations, \n",
    "    n_episodes_per_update=n_episodes_per_update, \n",
    "    n_max_steps=n_max_steps, \n",
    "    discount_factor=discount_factor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "(5,)\n",
      "(5, 1)\n",
      "(1,)\n",
      "<tf.Variable 'dense/kernel:0' shape=(4, 5) dtype=float32, numpy=\n",
      "array([[ 4.9890053e-01,  4.1784239e-01, -1.3949366e-02,  5.6299765e-02,\n",
      "        -6.8623363e-04],\n",
      "       [-6.7938656e-01,  8.4538913e-01,  1.2838671e-01, -2.2695495e-01,\n",
      "         1.3203785e-01],\n",
      "       [-1.2676984e+00, -4.9135035e-01,  1.6869855e+00,  7.1497774e-01,\n",
      "        -5.8706723e-02],\n",
      "       [-1.4064280e+00, -2.3572481e-01,  6.0852188e-01,  1.3012432e+00,\n",
      "         4.0517163e-01]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "for x in model.trainable_variables:\n",
    "    print(x.shape)\n",
    "\n",
    "model.trainable_variables == model.trainable_weights\n",
    "\n",
    "print(model.trainable_weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, env, max_steps):\n",
    "    obs, info = env.reset()\n",
    "    frames = [env.render()]\n",
    "    for step in range(max_steps):\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = int(tf.random.uniform([1, 1]) > left_proba)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        frames.append(env.render())\n",
    "        if done or truncated:\n",
    "            break\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(file_path, model, env, n_max_steps, override=False):\n",
    "    if not override and Path(file_path).exists():\n",
    "        return False\n",
    "    frames = play_game(model, env, n_max_steps)\n",
    "    imageio.mimsave(file_path, frames, format='GIF', duration=20)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_gif('data/cart.gif', model, env, n_max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad model :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "bad_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "bad_model = train_or_load_model(\n",
    "    bad_model, \n",
    "    'data/02-policy-bad/01',\n",
    "    n_iterations=10, \n",
    "    n_episodes_per_update=n_episodes_per_update, \n",
    "    n_max_steps=n_max_steps, \n",
    "    discount_factor=discount_factor, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_gif('data/bad_cart.gif', bad_model, env, n_max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reflects the graph in figure 18-8\n",
    "# shape = (s, a, s')\n",
    "transition_probabilities = [\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], None,            [0.0, 0.0, 1.0]],\n",
    "    [None,            [0.8, 0.1, 0.1], None],\n",
    "]\n",
    "\n",
    "# shape = (s, a, s')\n",
    "rewards = [\n",
    "    [[10, 0, 0], [0, 0, 0],  [0, 0, 0]],\n",
    "    [[0, 0, 0],  [0, 0, 0],  [0, 0, -50]],\n",
    "    [[0, 0, 0],  [40, 0, 0], [0, 0, 0]],\n",
    "]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-value iteration aglorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we know the transition probabilities and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.91891892 17.02702702 13.62162162]\n",
      " [ 0.                -inf -4.87971488]\n",
      " [       -inf 50.13365013        -inf]]\n"
     ]
    }
   ],
   "source": [
    "Q_values = np.full(shape=(3, 3), fill_value=-np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0\n",
    "\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.90\n",
    "\n",
    "for iterations in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp] \n",
    "                * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
    "                for sp in range(3) \n",
    "            ])\n",
    "\n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we don't know the transition probabilities or rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-values with zeroes\n",
    "Q_values = np.full(shape=(3, 3), fill_value=-np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.3441667  15.94753638 13.03186405]\n",
      " [ 0.                -inf -9.44323714]\n",
      " [       -inf 48.30531401        -inf]]\n"
     ]
    }
   ],
   "source": [
    "# Fully random\n",
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "# Get the next state and immediate reward based on action taken\n",
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward\n",
    "\n",
    "alpha0 = 0.05  # initial learning rate\n",
    "decay = 0.005  # learning rate decay\n",
    "gamma = 0.90   # discount factor\n",
    "state = 0      # initial state\n",
    "\n",
    "for iteration in range(10_000):\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    # Greedy policy at next step\n",
    "    next_value = Q_values[next_state].max()\n",
    "    # This decay approach is similar to power-scheduling\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state\n",
    "\n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the cart pole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN example for the cart pole problem\n",
    "from collections import deque\n",
    "\n",
    "# env.observation_space.shape\n",
    "input_shape = [4]\n",
    "# env.action_space.n\n",
    "n_outputs = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='elu', input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=False)[0]\n",
    "        # Optimal action according to the DQN\n",
    "        return Q_values.argmax()\n",
    "    \n",
    "\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    # Similar to `batch = replay_buffer[indices]` had replay_buffer been an ndarray\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    # Similar to batch.T had it been an ndarray. Returns:\n",
    "    # [states, actions, rewards, next_states, dones, truncateds]\n",
    "    return [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(6)\n",
    "    ]\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    replay_buffer.append([state, action, reward, next_state, done, truncated])\n",
    "    return next_state, reward, done, truncated, info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function _BaseOptimizer._update_step_xla at 0x7f5bb9fab100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 41 calls to <function _BaseOptimizer._update_step_xla at 0x7f5bb9fab100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    # (a) Take a batch of experiences from the replay buffer. This is of shape (6, 32)\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    # Each of these is of shape (32,)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    # (b) Calculate the targets for the DQN\n",
    "    # Step 1 - predict the next Q(s', a'). \n",
    "    # Return shape: (32, 2) as there are 2 outputs to the DQN.\n",
    "    # The values mean the approximate reward for each action in the next step s'.\n",
    "    next_Q_values = model.predict(next_states, verbose=False)\n",
    "    # Step 2 - Find Max.Q(s', a'), i.e. the max reward that can be achieved in \n",
    "    # step s'. This is the assumption the agent is acting optimally.\n",
    "    # Return shape: (32, 1)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    # Step 3 - build a (32, 1) binary mask where 1 indicates the episode is not over\n",
    "    # and 0 otherwise, ensuring no future rewards are considered for terminal states.\n",
    "    runs = 1.0 - (dones | truncateds)  \n",
    "    # Step 4 - Calculate the target value for the DQN, of shape (32, 1)\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    # (c) Calculate a mask used to extract the reward of the action actually taken\n",
    "    # This is a (32, 2) matrix, each containing a one-hot encoded vector representing\n",
    "    # the action that the agent took.\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    # (d) run one forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Recall that model.predict() does not contribute to recording operations in\n",
    "        # GradientTape context, which is why model() is used. \n",
    "        # Return value is of shape (32, 2)\n",
    "        all_Q_values = model(states)\n",
    "        # Calculate the estimated reward of the action actually taken, including discounted\n",
    "        # future rewards. Returns shape (32, 1)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        # Returns a scalar of the mean loss across samples \n",
    "        # (each sample's loss is MSE vs. target Q values).\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    # (e) Run one backprop pass \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model_folder = Path() / 'data' / '03-dqn'\n",
    "if model_folder.exists():\n",
    "    model = tf.keras.models.load_model(model_folder)\n",
    "else:\n",
    "    for episode in range(600):\n",
    "        obs, info = env.reset()\n",
    "        for step in range(200):\n",
    "            # For the -greedy policy, decay  linearly from 1 to 0.01 within the first 500 episodes\n",
    "            epsilon = max(1 - episode / 500, 0.01)\n",
    "            obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        # Give the replay buffer some time to fill up\n",
    "        if episode > 50:\n",
    "            training_step(batch_size)\n",
    "\n",
    "    model.save(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_dqn_game(model, env, max_steps):\n",
    "    obs, info = env.reset()\n",
    "    frames = [env.render()]\n",
    "    for step in range(max_steps):\n",
    "        actions = model.predict(obs[np.newaxis], verbose=False)[0]\n",
    "        action = actions.argmax()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        frames.append(env.render())\n",
    "        if done or truncated:\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "def create_dqn_gif(file_path, model, env, n_max_steps, override=False):\n",
    "    if not override and Path(file_path).exists():\n",
    "        return False\n",
    "    frames = play_dqn_game(model, env, n_max_steps)\n",
    "    print(len(frames))\n",
    "    imageio.mimsave(file_path, frames, format='GIF', duration=20)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dqn_gif('data/cart-dqn.gif', model, env, n_max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='elu', input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "target = tf.keras.models.clone_model(model)  # clone the model's architecture\n",
    "target.set_weights(model.get_weights())  # copy the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    # Instead of target.predict() like in the Fixed Q-value targets approach.\n",
    "    # This returns (32, 2) of indices of the best actions the online model predicts\n",
    "    # - not the target model's Q-values like before...\n",
    "    next_Q_values = model.predict(next_states, verbose=False)\n",
    "    # ...we then use these indices to fetch the target model's Q-values for each action,\n",
    "    # and use them as the online model's targets as usual\n",
    "    best_next_actions = next_Q_values.argmax(axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    max_next_Q_values = (target.predict(next_states, verbose=False) * next_mask).sum(axis=1)\n",
    "    # The rest is the same...\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    runs = 1.0 - (dones | truncateds)  \n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model_folder = Path() / 'data' / '04-double-dqn'\n",
    "if model_folder.exists():\n",
    "    model = tf.keras.models.load_model(model_folder)\n",
    "else:\n",
    "    for episode in range(600):\n",
    "        obs, info = env.reset()\n",
    "        for step in range(200):\n",
    "            # For the -greedy policy, decay  linearly from 1 to 0.01 within the first 500 episodes\n",
    "            epsilon = max(1 - episode / 500, 0.01)\n",
    "            obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        # Give the replay buffer some time to fill up\n",
    "        if episode > 50:\n",
    "            training_step(batch_size)\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            target.set_weights(model.get_weights())    \n",
    "\n",
    "    model.save(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dqn_gif('data/cart-double-dqn.gif', model, env, n_max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual DQN (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_states = tf.keras.layers.Input(shape=[4])\n",
    "hidden1 = tf.keras.layers.Dense(32, activation='elu')(input_states)\n",
    "hidden2 = tf.keras.layers.Dense(32, activation='elu')(hidden1)\n",
    "# V(s)\n",
    "state_values = tf.keras.layers.Dense(1)(hidden2)\n",
    "# A(s, a) before standardizing\n",
    "raw_advantages = tf.keras.layers.Dense(n_outputs)(hidden2)\n",
    "# A(s, a) after standardizing, ensuring A(s, a*) is 0 and all the rest are negative\n",
    "advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1, keepdims=True)\n",
    "# Q(s, a) = V(s) + A(s, a)\n",
    "Q_values = state_values + advantages\n",
    "model = tf.keras.Model(inputs=[input_states], outputs=[Q_values])\n",
    "\n",
    "target = tf.keras.models.clone_model(model)  # clone the model's architecture\n",
    "target.set_weights(model.get_weights())  # copy the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = model.predict(next_states, verbose=False)\n",
    "    best_next_actions = next_Q_values.argmax(axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    max_next_Q_values = (target.predict(next_states, verbose=False) * next_mask).sum(axis=1)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    runs = 1.0 - (dones | truncateds)  \n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/05-ddqn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/05-ddqn/assets\n"
     ]
    }
   ],
   "source": [
    "model_folder = Path() / 'data' / '05-ddqn'\n",
    "if model_folder.exists():\n",
    "    model = tf.keras.models.load_model(model_folder)\n",
    "else:\n",
    "    for episode in range(600):\n",
    "        obs, info = env.reset()\n",
    "        for step in range(200):\n",
    "            # For the -greedy policy, decay  linearly from 1 to 0.01 within the first 500 episodes\n",
    "            epsilon = max(1 - episode / 500, 0.01)\n",
    "            obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        # Give the replay buffer some time to fill up\n",
    "        if episode > 50:\n",
    "            training_step(batch_size)\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            target.set_weights(model.get_weights())    \n",
    "\n",
    "    model.save(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dqn_gif('data/cart-ddqn.gif', model, env, n_max_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
