{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "a. Download the Large Movie Review Dataset, which contains 50,000 movie reviews from the Internet Movie Database (IMDb). The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words versions), but we will ignore them in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b. Split the test set into a validation set (15,000) and a test set (10,000)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T15:01:02.289909Z",
     "start_time": "2023-06-25T15:00:59.295841Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First attempt: Bad approach\n",
    "\n",
    "This is not a good approach. A better approach is to simply split the files in the folder to validation and test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "test_pos_files = tf.data.Dataset.list_files('data/aclImdb/test/pos/*.txt')\n",
    "test_neg_files = tf.data.Dataset.list_files('data/aclImdb/test/neg/*.txt')\n",
    "\n",
    "def attach_label(label):\n",
    "    def _attach_label(x):\n",
    "        return x, tf.constant([label], dtype=tf.int64)\n",
    "    return _attach_label\n",
    "\n",
    "test_pos = tf.data.TextLineDataset(test_pos_files).map(attach_label(1))\n",
    "test_neg = tf.data.TextLineDataset(test_neg_files).map(attach_label(0))\n",
    "test_full: tf.data.Dataset = test_pos.concatenate(test_neg).shuffle(25000, seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T16:03:29.321297Z",
     "start_time": "2023-06-25T16:03:29.075565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "valid_input_arr = []\n",
    "valid_label_arr = []\n",
    "test_input_arr = []\n",
    "test_label_arr = []\n",
    "for index, (input, label) in test_full.enumerate():\n",
    "    if index < 15000:\n",
    "        valid_input_arr.append(input)\n",
    "        valid_label_arr.append(label)\n",
    "    else:\n",
    "        test_input_arr.append(input)\n",
    "        test_label_arr.append(label)\n",
    "\n",
    "valid: tf.data.Dataset = tf.data.Dataset.from_tensor_slices((valid_input_arr, valid_label_arr))\n",
    "test: tf.data.Dataset = tf.data.Dataset.from_tensor_slices((test_input_arr, test_label_arr))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T15:49:02.790154Z",
     "start_time": "2023-06-25T15:48:52.705883Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second attempt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 7500 5000 5000 12500 12500\n"
     ]
    }
   ],
   "source": [
    "test_pos_files = tf.data.Dataset.list_files('data/aclImdb/test/pos/*.txt', shuffle=False)\n",
    "test_neg_files = tf.data.Dataset.list_files('data/aclImdb/test/neg/*.txt', shuffle=False)\n",
    "train_pos_files = tf.data.Dataset.list_files('data/aclImdb/train/pos/*.txt', shuffle=False)\n",
    "train_neg_files = tf.data.Dataset.list_files('data/aclImdb/train/pos/*.txt', shuffle=False)\n",
    "\n",
    "test_pos_files = [x.numpy() for x in test_pos_files]\n",
    "test_neg_files = [x.numpy() for x in test_neg_files]\n",
    "\n",
    "valid_pos_files, test_pos_files = test_pos_files[:7500], test_pos_files[7500:]\n",
    "valid_neg_files, test_neg_files = test_neg_files[:7500], test_neg_files[7500:]\n",
    "\n",
    "print(\n",
    "    len(valid_pos_files),\n",
    "    len(valid_neg_files),\n",
    "    len(test_pos_files),\n",
    "    len(test_neg_files),\n",
    "    len(train_pos_files),\n",
    "    len(train_neg_files),\n",
    ")\n",
    "\n",
    "def attach_label(label):\n",
    "    def _attach_label(x):\n",
    "        return x, label\n",
    "    return _attach_label\n",
    "\n",
    "valid_pos = tf.data.TextLineDataset(valid_pos_files, num_parallel_reads=5).map(attach_label(1))\n",
    "valid_neg = tf.data.TextLineDataset(valid_neg_files, num_parallel_reads=5).map(attach_label(0))\n",
    "test_pos = tf.data.TextLineDataset(test_pos_files, num_parallel_reads=5).map(attach_label(1))\n",
    "test_neg = tf.data.TextLineDataset(test_neg_files, num_parallel_reads=5).map(attach_label(0))\n",
    "train_pos = tf.data.TextLineDataset(train_pos_files, num_parallel_reads=5).map(attach_label(1))\n",
    "train_neg = tf.data.TextLineDataset(train_neg_files, num_parallel_reads=5).map(attach_label(0))\n",
    "\n",
    "valid = valid_pos.concatenate(valid_neg).batch(32).prefetch(1)\n",
    "test = test_pos.concatenate(test_neg).batch(32).prefetch(1)\n",
    "train = train_pos.concatenate(train_neg).shuffle(25000).batch(32).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:26:26.471116Z",
     "start_time": "2023-06-25T18:26:25.077543Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c. Use tf.data to create an efficient dataset for each set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "d. Create a binary classification model, using a TextVectorization layer to preprocess each review."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "vectorization = tf.keras.layers.TextVectorization(output_mode='tf_idf', max_tokens=1000)\n",
    "vectorization.adapt(train.concatenate(valid).concatenate(test).map(lambda x, label: x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T16:48:57.502891Z",
     "start_time": "2023-06-25T16:48:15.007301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'br', 'as', 'was', 'with', 'for', 'but', 'movie', 'film']\n",
      "['laughs', 'whatever', 'members', 'sounds', 'lee', 'beautifully', 'reasons', 'popular', 'secret', '20', 'otherwise', 'box', 'appear', 'minute', 'moves', 'apart', 'uses', 'credits', 'front', 'large']\n"
     ]
    }
   ],
   "source": [
    "print(vectorization.get_vocabulary()[:20])\n",
    "print(vectorization.get_vocabulary()[980:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:33:40.666746Z",
     "start_time": "2023-06-25T18:33:40.645094Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization.vocabulary_size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:33:43.548354Z",
     "start_time": "2023-06-25T18:33:43.544923Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3.012064 0.       0.       0.       0.      ], shape=(5,), dtype=float32)\n",
      "tf.Tensor([3.012064  0.6979414 0.        0.        0.       ], shape=(5,), dtype=float32)\n",
      "tf.Tensor([3.012064  0.6979414 0.7099822 0.        0.       ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings of a sentence seems to simply add the weights for each word\n",
    "print(vectorization('asdfasdf')[:5])\n",
    "print(vectorization('asdfasdf the')[:5])\n",
    "print(vectorization('asdfasdf the and')[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:33:45.975535Z",
     "start_time": "2023-06-25T18:33:45.946729Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 18s 20ms/step - loss: 0.8913 - binary_accuracy: 0.4657 - val_loss: 0.9073 - val_binary_accuracy: 0.4963\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 16s 19ms/step - loss: 0.7732 - binary_accuracy: 0.4567 - val_loss: 0.7755 - val_binary_accuracy: 0.4956\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 15s 18ms/step - loss: 0.7573 - binary_accuracy: 0.4660 - val_loss: 0.7229 - val_binary_accuracy: 0.4657\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 15s 18ms/step - loss: 0.7490 - binary_accuracy: 0.4566 - val_loss: 0.7263 - val_binary_accuracy: 0.4788\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.7422 - binary_accuracy: 0.4573 - val_loss: 0.7009 - val_binary_accuracy: 0.5047\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    vectorization,\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.legacy.Nadam(learning_rate=0.0005),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "hist = model.fit(train, epochs=5, validation_data=valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:36:16.553317Z",
     "start_time": "2023-06-25T18:34:57.833163Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "e. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> An embeddings layer starts with a sparse categorical value (a number between 0 and max_tokens). But here, the solution (which I read only up to here) suggests TF-IDF, which produces hot encoded vector. Matrix multiplication between the tf-idf-hot encoded vectorization layer and the embedding layer (dense layer) will essentially take care of \"adding the vectors\" part. But what about the square root of the number of words? My instinct is to create a custom layer that for a given input tf-idf-hot encoded matrix X, it performs this \"normalization\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[[0.5, 0.5], [0.5, 0.5]]"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x1 = np.ones((2, 2))\n",
    "x2 = np.zeros((2, 2))\n",
    "print(x2)\n",
    "y = tf.keras.layers.Average()([x1, x2])\n",
    "y.numpy().tolist()\n",
    "\n",
    "tf.keras.layers."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T17:26:00.723347Z",
     "start_time": "2023-06-25T17:26:00.670930Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "f. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
