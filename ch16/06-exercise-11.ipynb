{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Hugging Face Transformers library to download a pretrained language model capable of generating text (e.g., GPT), and try generating more convincing Shakespearean text. You will need to use the model’s generate() method—see Hugging Face’s documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorrt\n",
    "import tensorflow as tf\n",
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "from transformers import OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFOpenAIGPTLMHeadModel: ['h.3.attn.bias', 'h.6.attn.bias', 'h.1.attn.bias', 'h.0.attn.bias', 'h.10.attn.bias', 'h.9.attn.bias', 'h.8.attn.bias', 'h.4.attn.bias', 'h.11.attn.bias', 'h.2.attn.bias', 'h.7.attn.bias', 'h.5.attn.bias']\n",
      "- This IS expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFOpenAIGPTLMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030eacfa9c1b4905994d80a09222600b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab459524ffe14aea85765268826a3e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [616, 5751], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"This royal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   498,   973, 32000,   980,  5705,   725,   488,   725,\n",
       "         6560,   498,   246,  5908,   557,   618,   239,  1288,  7232,\n",
       "        15969,   759,   808,  3725,   246, 18493,   498,   754, 23181,\n",
       "          270,   488,   589,   525,   481,  1849,   618,   509,   817,\n",
       "          485, 27790,   498,   544,   481],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   488,   616, 16499,   240,   645,   507,  2864,\n",
       "          551,   525,   249,   604,   595,  5476,  1311,  1295,   246,\n",
       "         1796,   500,   481,  1030,   498,   481,  7232,  7339,   498,\n",
       "          481,  1175,   498,   481,  2761,   240,   812,  4103,   664,\n",
       "         4001,   562,   566,   620,  9500],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239,   645,   524,  7455,   626,   595, 18645,   240,\n",
       "          487,   636,  4840,   524,  2080,   239,   246,  7047,   636,\n",
       "          580,  5486,   240,   568,   595,   246,  7047,   240,   595,\n",
       "          566,   498,   246,   618,   535,   239,   481,  2565,   498,\n",
       "          867,  9606,   635,  1315,   522],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,  2191,  1760,   485,   510,   240,   512,   587,   595,\n",
       "          966,   485,   925,   246,  4978,   485,   507,   240,   889,\n",
       "         1101,   803,  7227,  1983,   498,  6404,   239,   664,   240,\n",
       "          595,  1198,   562,   512,   488,   704, 11709,   240,   246,\n",
       "         6404,   525,   512,   759,  1074],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   556,   481,  9606, 11828,   240,   491,   481,\n",
       "         8077,   498,   481,   776,   249,  2455,  4332,   239,   507,\n",
       "          509,   246,  4423,   850,   240,   488,   481,  1436,    21,\n",
       "          982,   254,  2573,  2418,   500,   524,  5625,   240,   524,\n",
       "          928,  1133, 17788,  7662,   239]], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle of pidaria has proved more and more worthy of a hero as king. these mighty vessels can only carry a token of their achievement ; and all that the true king was able to boast of is the\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, and this voyage, if it turns out that i have not thus far seen a change in the ways of the mighty kingdom of the men of the sea, will serve no purpose for one so defeated\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle. if his empire did not yield, he would destroy his children. a crown would be required, but not a crown, not one of a king's. the promise of two kings could mean or\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle makes sense to me, you do not need to make a claim to it, much less some impressive form of throne. no, not better for you and your subjects, a throne that you can own\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, with the kings assembled, at the centre of the gai tree garden. it was a winter day, and the god bheu arrived early in his robe, his long hair braided neatly.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
