{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "a. Download the Large Movie Review Dataset, which contains 50,000 movie reviews from the Internet Movie Database (IMDb). The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words versions), but we will ignore them in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b. Split the test set into a validation set (15,000) and a test set (10,000)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 17:25:16.558411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorrt\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:25:18.655659900Z",
     "start_time": "2023-06-26T15:25:15.702657300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First attempt: Bad approach\n",
    "\n",
    "This is not a good approach. A better approach is to simply split the files in the folder to validation and test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 13:01:48.152912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:48.195041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:48.195103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:48.198627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:48.198706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:48.198739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:49.104810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:49.105155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:49.105167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-06-26 13:01:49.105214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 13:01:49.105250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "test_pos_files = tf.data.Dataset.list_files('data/aclImdb/test/pos/*.txt')\n",
    "test_neg_files = tf.data.Dataset.list_files('data/aclImdb/test/neg/*.txt')\n",
    "\n",
    "def attach_label(label):\n",
    "    def _attach_label(x):\n",
    "        return x, tf.constant([label], dtype=tf.int64)\n",
    "    return _attach_label\n",
    "\n",
    "test_pos = tf.data.TextLineDataset(test_pos_files).map(attach_label(1))\n",
    "test_neg = tf.data.TextLineDataset(test_neg_files).map(attach_label(0))\n",
    "test_full: tf.data.Dataset = test_pos.concatenate(test_neg).shuffle(25000, seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T11:23:33.909500200Z",
     "start_time": "2023-06-26T11:23:33.072453100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 13:01:50.024858: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2023-06-26 13:01:50.032742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-06-26 13:01:50.033087: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_13' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_13}}]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_11283/1486600809.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mtest_input_arr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0mtest_label_arr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0mvalid\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid_input_arr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_label_arr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_input_arr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_label_arr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(tensors, name)\u001B[0m\n\u001B[1;32m    826\u001B[0m     \u001B[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    827\u001B[0m     \u001B[0;31m# from_tensor_slices_op -> dataset_ops).\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    828\u001B[0m     \u001B[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    829\u001B[0m     \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mops\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfrom_tensor_slices_op\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 830\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mfrom_tensor_slices_op\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_from_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    831\u001B[0m     \u001B[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(tensors, name)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_from_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0m_TensorSliceDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, element, is_files, name)\u001B[0m\n\u001B[1;32m     31\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0melement\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_files\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0;34m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m     \u001B[0melement\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize_element\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m     \u001B[0mbatched_spec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype_spec_from_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tensors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_batched_tensor_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatched_spec\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0melement\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tensors\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(element, element_signature)\u001B[0m\n\u001B[1;32m    129\u001B[0m           \u001B[0mnormalized_components\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m           \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspec\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"dtype\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m           normalized_components.append(\n\u001B[0;32m--> 133\u001B[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001B[0m\u001B[1;32m    134\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpack_sequence_as\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpack_as\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnormalized_components\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0menabled\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    181\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrace_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mtrace_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m           \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 183\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[1;32m   1638\u001B[0m                   \u001B[0;34mf\"actual = {ret.dtype.base_dtype.name}\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1639\u001B[0m                   name=name))\n\u001B[1;32m   1640\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1641\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1642\u001B[0;31m       \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconversion_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mas_ref\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1643\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1644\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mNotImplemented\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1645\u001B[0m       \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(v, dtype, name, as_ref)\u001B[0m\n\u001B[1;32m   1603\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1604\u001B[0m     \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minferred_dtype\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1605\u001B[0m   \u001B[0;32melif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0minferred_dtype\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1606\u001B[0m     \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_cast_nested_seqs_to_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1607\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0m_autopacking_helper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m\"packed\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(list_or_tuple, dtype, name)\u001B[0m\n\u001B[1;32m   1510\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1511\u001B[0m     \u001B[0;31m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1512\u001B[0m     \u001B[0;31m# checking.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1513\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0melem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlist_or_tuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1514\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist_or_tuple\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1515\u001B[0m   \u001B[0mmust_pack\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1516\u001B[0m   \u001B[0mconverted_elems\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m   \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mscope\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/ops/gen_array_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(values, axis, name)\u001B[0m\n\u001B[1;32m   6574\u001B[0m         _ctx, \"Pack\", name, values, \"axis\", axis)\n\u001B[1;32m   6575\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6576\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6577\u001B[0m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6578\u001B[0;31m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6579\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6580\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6581\u001B[0m       return pack_eager_fallback(\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "valid_input_arr = []\n",
    "valid_label_arr = []\n",
    "test_input_arr = []\n",
    "test_label_arr = []\n",
    "for index, (input, label) in test_full.enumerate():\n",
    "    if index < 15000:\n",
    "        valid_input_arr.append(input)\n",
    "        valid_label_arr.append(label)\n",
    "    else:\n",
    "        test_input_arr.append(input)\n",
    "        test_label_arr.append(label)\n",
    "\n",
    "valid: tf.data.Dataset = tf.data.Dataset.from_tensor_slices((valid_input_arr, valid_label_arr))\n",
    "test: tf.data.Dataset = tf.data.Dataset.from_tensor_slices((test_input_arr, test_label_arr))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T11:23:50.277087200Z",
     "start_time": "2023-06-26T11:23:33.912500400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second attempt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 17:25:23.971366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:24.119309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:24.119391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:24.127657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:24.127743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:24.127765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:25.331600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:25.331650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:25.331657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-06-26 17:25:25.331675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-26 17:25:25.331701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-06-26 17:25:25.887654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-26 17:25:27.054905: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 7500 5000 5000 12500 12500\n"
     ]
    }
   ],
   "source": [
    "test_pos_files = tf.data.Dataset.list_files('data/aclImdb/test/pos/*.txt', shuffle=False)\n",
    "test_neg_files = tf.data.Dataset.list_files('data/aclImdb/test/neg/*.txt', shuffle=False)\n",
    "train_pos_files = tf.data.Dataset.list_files('data/aclImdb/train/pos/*.txt', shuffle=False)\n",
    "train_neg_files = tf.data.Dataset.list_files('data/aclImdb/train/neg/*.txt', shuffle=False)\n",
    "\n",
    "test_pos_files = [x.numpy() for x in test_pos_files]\n",
    "test_neg_files = [x.numpy() for x in test_neg_files]\n",
    "\n",
    "test_pos_files, valid_pos_files = test_pos_files[:5000], test_pos_files[5000:]\n",
    "test_neg_files, valid_neg_files = test_neg_files[:5000], test_neg_files[5000:]\n",
    "\n",
    "print(\n",
    "    len(valid_pos_files),\n",
    "    len(valid_neg_files),\n",
    "    len(test_pos_files),\n",
    "    len(test_neg_files),\n",
    "    len(train_pos_files),\n",
    "    len(train_neg_files),\n",
    ")\n",
    "\n",
    "def attach_label(label):\n",
    "    def _attach_label(x):\n",
    "        return x, label\n",
    "    return _attach_label\n",
    "\n",
    "valid_pos = tf.data.TextLineDataset(valid_pos_files, num_parallel_reads=5).map(attach_label(1))\n",
    "valid_neg = tf.data.TextLineDataset(valid_neg_files, num_parallel_reads=5).map(attach_label(0))\n",
    "test_pos = tf.data.TextLineDataset(test_pos_files, num_parallel_reads=5).map(attach_label(1))\n",
    "test_neg = tf.data.TextLineDataset(test_neg_files, num_parallel_reads=5).map(attach_label(0))\n",
    "train_pos = tf.data.TextLineDataset(train_pos_files, num_parallel_reads=5).map(attach_label(1))\n",
    "train_neg = tf.data.TextLineDataset(train_neg_files, num_parallel_reads=5).map(attach_label(0))\n",
    "\n",
    "valid = valid_pos.concatenate(valid_neg).batch(32).prefetch(1)\n",
    "test = test_pos.concatenate(test_neg).batch(32).prefetch(1)\n",
    "train = train_pos.concatenate(train_neg).shuffle(25000).batch(32).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:25:28.352245200Z",
     "start_time": "2023-06-26T15:25:25.450600200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c. Use tf.data to create an efficient dataset for each set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "d. Create a binary classification model, using a TextVectorization layer to preprocess each review."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1: without embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 17:25:34.028732: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n",
      "2023-06-26 17:25:34.029079: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n"
     ]
    }
   ],
   "source": [
    "vectorization = tf.keras.layers.TextVectorization(output_mode='tf_idf', max_tokens=1000)\n",
    "vectorization.adapt(train.map(lambda x, label: x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:25:54.028644300Z",
     "start_time": "2023-06-26T15:25:33.879438500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but', 'film']\n",
      "['ideas', 'expecting', 'jane', 'fails', 'deserves', 'present', 'political', 'missing', 'attempts', 'twist', 'secret', 'fire', 'dumb', 'unlike', 'fighting', 'fantasy', 'pay', 'air', 'joke', 'gay']\n"
     ]
    }
   ],
   "source": [
    "print(vectorization.get_vocabulary()[:20])\n",
    "print(vectorization.get_vocabulary()[980:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:31:38.553491400Z",
     "start_time": "2023-06-26T15:31:38.491688200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization.vocabulary_size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T12:28:04.819286400Z",
     "start_time": "2023-06-26T12:28:04.760776600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2.9993966 0.        0.        0.        0.       ], shape=(5,), dtype=float32)\n",
      "tf.Tensor([2.9993966  0.69735354 0.         0.         0.        ], shape=(5,), dtype=float32)\n",
      "tf.Tensor([2.9993966  0.69735354 0.7110562  0.         0.        ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings of a sentence seems to simply add the weights for each word\n",
    "print(vectorization('asdfasdf')[:5])\n",
    "print(vectorization('asdfasdf the')[:5])\n",
    "print(vectorization('asdfasdf the and')[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:43:00.511668800Z",
     "start_time": "2023-06-26T15:43:00.440895800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.8815613]\n",
      " [0.5998793]], shape=(2, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[1.7320508],\n       [1.       ]], dtype=float32)>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x: tf.Tensor = vectorization('asdfasdf the and')[:5]\n",
    "y: tf.Tensor = vectorization('asdfadsfa')[:5]\n",
    "data = tf.stack([x, y], axis=0)\n",
    "mean = tf.reduce_mean(data, axis=1, keepdims=True)\n",
    "print(mean)\n",
    "word_count = tf.math.count_nonzero(data, axis=1, keepdims=True, dtype=tf.float32)\n",
    "tf.sqrt(word_count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:52:17.645289900Z",
     "start_time": "2023-06-26T15:52:17.618153300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 17:31:42.530908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n",
      "2023-06-26 17:31:42.531166: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n",
      "2023-06-26 17:31:46.049511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-06-26 17:31:46.086953: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fd6e41b1740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-26 17:31:46.086992: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-06-26 17:31:46.122426: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-26 17:31:48.535571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2023-06-26 17:31:48.703982: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-06-26 17:31:48.799548: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    777/Unknown - 11s 6ms/step - loss: 0.4275 - binary_accuracy: 0.8191"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 17:31:53.995078: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [7500]\n",
      "\t [[{{node Placeholder/_8}}]]\n",
      "2023-06-26 17:31:53.995329: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [7500]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 9ms/step - loss: 0.4274 - binary_accuracy: 0.8191 - val_loss: 0.4084 - val_binary_accuracy: 0.8333\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.3618 - binary_accuracy: 0.8527 - val_loss: 0.3720 - val_binary_accuracy: 0.8443\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.3191 - binary_accuracy: 0.8676 - val_loss: 0.3506 - val_binary_accuracy: 0.8512\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.2609 - binary_accuracy: 0.8942 - val_loss: 0.3784 - val_binary_accuracy: 0.8452\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.2011 - binary_accuracy: 0.9216 - val_loss: 0.4085 - val_binary_accuracy: 0.8350\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    vectorization,\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "hist = model.fit(train, epochs=5, validation_data=valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:32:27.140139900Z",
     "start_time": "2023-06-26T15:31:42.464103600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2: My custom of embeddings using hot-vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "e. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> An embeddings layer starts with a sparse categorical value (a number between 0 and max_tokens). But here, the solution (which I read only up to here) suggests TF-IDF, which produces hot encoded vector. Matrix multiplication between the tf-idf-hot encoded vectorization layer and the embedding layer (dense layer) will essentially take care of \"adding the vectors\" part. But what about the square root of the number of words? My instinct is to create a custom layer that for a given input tf-idf-hot encoded matrix X, it performs this \"normalization\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class MyEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            'kernel',\n",
    "            shape=(input_shape[-1], self.output_dim),\n",
    "            dtype=tf.float32,\n",
    "            initializer='he_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        # self.bias = self.add_weight(\n",
    "        #     'bias',\n",
    "        #     shape=[self.output_dim],\n",
    "        #     dtype=tf.float32,\n",
    "        #     trainable=True\n",
    "        # )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        word_count = tf.math.count_nonzero(inputs, axis=1, keepdims=True, dtype=tf.float32)\n",
    "        return (inputs @ self.kernel) / tf.sqrt(word_count)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return { **base_config, 'output_dim': self.output_dim }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T16:32:16.788757200Z",
     "start_time": "2023-06-26T16:32:16.737858100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 10s 10ms/step - loss: 0.3770 - binary_accuracy: 0.8359 - val_loss: 0.3302 - val_binary_accuracy: 0.8557\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 9s 10ms/step - loss: 0.3240 - binary_accuracy: 0.8606 - val_loss: 0.3310 - val_binary_accuracy: 0.8567\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.3030 - binary_accuracy: 0.8672 - val_loss: 0.3264 - val_binary_accuracy: 0.8593\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 9s 10ms/step - loss: 0.2804 - binary_accuracy: 0.8775 - val_loss: 0.3443 - val_binary_accuracy: 0.8541\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 9s 10ms/step - loss: 0.2538 - binary_accuracy: 0.8924 - val_loss: 0.3590 - val_binary_accuracy: 0.8497\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = tf.keras.models.Sequential([\n",
    "    vectorization,\n",
    "    MyEmbedding(300)\n",
    "])\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    embeddings_model,\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "hist = model.fit(train, epochs=5, validation_data=valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T16:33:04.823271700Z",
     "start_time": "2023-06-26T16:32:19.742907900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "queen = embeddings_model.predict(['queen'])\n",
    "king = embeddings_model.predict(['king'])\n",
    "man = embeddings_model.predict(['man'])\n",
    "woman = embeddings_model.predict(['woman'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T16:22:17.970677500Z",
     "start_time": "2023-06-26T16:22:17.781310600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def distance(x, y):\n",
    "    return np.sqrt((x - y) @ (x - y).T)[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T16:22:40.718563600Z",
     "start_time": "2023-06-26T16:22:40.714432200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1395469\n",
      "2.8197706\n",
      "1.9886863\n",
      "1.9030378\n",
      "3.5110815\n"
     ]
    }
   ],
   "source": [
    "print(distance(king, man))\n",
    "print(distance(king, queen))\n",
    "print(distance(queen, man))\n",
    "print(distance(queen, woman))\n",
    "print(distance(king - man + woman, queen))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T16:23:24.777892700Z",
     "start_time": "2023-06-26T16:23:24.756360700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 3: Keras embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 19:15:16.963051: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-26 19:15:16.963332: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1000\n",
    "vectorization_layer = tf.keras.layers.TextVectorization(max_tokens=max_tokens, output_mode='int')\n",
    "sample_reviews = train.map(lambda x, label: x)\n",
    "vectorization_layer.adapt(sample_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T17:15:21.213308500Z",
     "start_time": "2023-06-26T17:15:16.790810600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Understanding the shape of the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The vectorization layer returns a vector of words per review"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  2   1   5 119], shape=(4,), dtype=int64)\n",
      "tf.Tensor([  9  14 179  50], shape=(4,), dtype=int64)\n",
      "tf.Tensor([  1 188], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "x = vectorization_layer('The meaning of life')\n",
    "y = vectorization_layer('It was pretty good')\n",
    "z = vectorization_layer('adafdasdf world')\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T17:26:05.713107700Z",
     "start_time": "2023-06-26T17:26:05.545193300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When a batch of sentences is involved, the padding token 0 is used to ensure the tensor has the same shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 9), dtype=int64, numpy=\narray([[  2,   1,   5, 119,   7,   4,  50,  18,   0],\n       [ 10, 117,  22,  39,  49, 562,   8,  27, 129],\n       [134,   1,   2, 172,  82,  11,   0,   0,   0]])>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\n",
    "    'The meaning of life is a good movie',\n",
    "    'I did not like what happened in he end',\n",
    "    'Why woudld the director do this'\n",
    "]\n",
    "\n",
    "batch = vectorization_layer(batch)\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T17:29:39.325637100Z",
     "start_time": "2023-06-26T17:29:39.273014100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An embeddings layer (randomly initialized) returns one vector output_dim-size per word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\narray([[ 0.02916148, -0.00272601, -0.03369595,  0.02627398,  0.03792412],\n       [ 0.01223432, -0.03561933,  0.01368679, -0.04005159, -0.02686861],\n       [ 0.01188274,  0.01561645, -0.03099689,  0.03140095, -0.03017026],\n       [-0.00366409, -0.00441701, -0.02873117, -0.00038002, -0.01314867]],\n      dtype=float32)>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_layer = tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=5, mask_zero=True)\n",
    "embeddings_layer(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:28:06.691702100Z",
     "start_time": "2023-06-26T19:28:06.642359500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 9, 5), dtype=float32, numpy=\narray([[[ 0.02916148, -0.00272601, -0.03369595,  0.02627398,\n          0.03792412],\n        [ 0.01223432, -0.03561933,  0.01368679, -0.04005159,\n         -0.02686861],\n        [ 0.01188274,  0.01561645, -0.03099689,  0.03140095,\n         -0.03017026],\n        [-0.00366409, -0.00441701, -0.02873117, -0.00038002,\n         -0.01314867],\n        [ 0.0154869 ,  0.01773483,  0.04219978,  0.02602367,\n         -0.00954236],\n        [-0.03284589, -0.01595576,  0.03316909, -0.02532237,\n         -0.03351758],\n        [-0.00601077,  0.02821207,  0.01040041,  0.02956816,\n          0.00224223],\n        [-0.00260069,  0.0118472 ,  0.02175767, -0.01263409,\n          0.03960122],\n        [ 0.00372756, -0.04539081, -0.00437624, -0.00571344,\n          0.04703838]],\n\n       [[-0.00243868, -0.01088219, -0.03810662, -0.02347549,\n         -0.02771819],\n        [ 0.04517824,  0.01395022,  0.03220079, -0.01081709,\n         -0.01430261],\n        [-0.01549876,  0.02083813,  0.03533414,  0.02597774,\n         -0.02972584],\n        [-0.01794503,  0.01362114,  0.00807006, -0.04368849,\n         -0.01644544],\n        [ 0.03082839,  0.04998343,  0.00280511,  0.0157407 ,\n         -0.03034815],\n        [ 0.00806385, -0.04348112, -0.03253865,  0.00262251,\n          0.02594247],\n        [-0.04792697, -0.00478839,  0.00356063, -0.00524352,\n          0.02164311],\n        [ 0.01810164,  0.04314072,  0.03352082, -0.02669095,\n         -0.00462613],\n        [-0.03970181,  0.02902714,  0.01054246, -0.03816034,\n          0.01663787]],\n\n       [[-0.02260004, -0.03715248,  0.04110725, -0.02489769,\n         -0.02603905],\n        [ 0.01223432, -0.03561933,  0.01368679, -0.04005159,\n         -0.02686861],\n        [ 0.02916148, -0.00272601, -0.03369595,  0.02627398,\n          0.03792412],\n        [ 0.0203331 , -0.03810508,  0.04358092,  0.0373962 ,\n         -0.01187851],\n        [ 0.04903729, -0.04601986,  0.04820042,  0.01389679,\n          0.0375444 ],\n        [ 0.01660513, -0.03082577, -0.02892648, -0.0127071 ,\n         -0.00535071],\n        [ 0.00372756, -0.04539081, -0.00437624, -0.00571344,\n          0.04703838],\n        [ 0.00372756, -0.04539081, -0.00437624, -0.00571344,\n          0.04703838],\n        [ 0.00372756, -0.04539081, -0.00437624, -0.00571344,\n          0.04703838]]], dtype=float32)>"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_batch = embeddings_layer(batch)\n",
    "embedded_batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:28:10.902706800Z",
     "start_time": "2023-06-26T19:28:10.851416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\narray([[ 0.00304128, -0.00341093,  0.0026015 ,  0.00324058,  0.0015065 ],\n       [-0.00237101,  0.01237879,  0.0061543 , -0.0115261 , -0.00654921],\n       [ 0.01288377, -0.03629122,  0.00786936, -0.00191442,  0.01627186]],\n      dtype=float32)>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(embedded_batch, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:28:20.237775100Z",
     "start_time": "2023-06-26T19:28:20.183561500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# The shape of x is (batch_size, longest_sequence, output_dim)\n",
    "# Our goal should be to transform it to (batch_size, output_dim)\n",
    "def compute_embeddings(x, mask=None):\n",
    "    return tf.reduce_mean(x, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T18:14:04.114892500Z",
     "start_time": "2023-06-26T18:14:04.111678500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 34s 40ms/step - loss: 0.5042 - binary_accuracy: 0.7461 - val_loss: 0.3683 - val_binary_accuracy: 0.8451\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 13s 15ms/step - loss: 0.3442 - binary_accuracy: 0.8554 - val_loss: 0.3345 - val_binary_accuracy: 0.8551\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 0.3205 - binary_accuracy: 0.8659 - val_loss: 0.3428 - val_binary_accuracy: 0.8493\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.3123 - binary_accuracy: 0.8679 - val_loss: 0.3305 - val_binary_accuracy: 0.8556\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.3071 - binary_accuracy: 0.8730 - val_loss: 0.3252 - val_binary_accuracy: 0.8592\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fd718b2e690>"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_embeddings = tf.keras.models.Sequential([\n",
    "    vectorization_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=100, mask_zero=True),\n",
    "])\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "    model_3_embeddings,\n",
    "    tf.keras.layers.Lambda(compute_embeddings),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_3.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "model_3.fit(train, epochs=5, validation_data=valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T18:17:10.758557700Z",
     "start_time": "2023-06-26T18:15:51.999775200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 1), dtype=int64, numpy=\narray([[9],\n       [9],\n       [9]])>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model_3_embeddings.predict([\n",
    "    'The meaning of life is a good movie',\n",
    "    'I did not like what happened in he end',\n",
    "    'Why woudld the director do this'\n",
    "])\n",
    "\n",
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
    "    # sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    # return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
    "    return n_words\n",
    "\n",
    "compute_mean_embedding(prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T18:44:23.243583100Z",
     "start_time": "2023-06-26T18:44:23.194569100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3 2 0]\n",
      " [1 0 0]], shape=(2, 3), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[2]\n",
      " [1]], shape=(2, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "input = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                     [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "not_pad = tf.math.count_nonzero(input, axis=-1)\n",
    "print(not_pad)\n",
    "n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
    "print(n_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T18:43:44.909361Z",
     "start_time": "2023-06-26T18:43:44.857811300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 4: My sentence embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  2   1   5 119   7   4  50  18   0]\n",
      " [ 10 117  22  39  49 562   8  27 129]\n",
      " [134   1   2 172  82  11   0   0   0]], shape=(3, 9), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\narray([[2.828427 ],\n       [3.       ],\n       [2.4494896]], dtype=float32)>"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2 = [\n",
    "    'The meaning of life is a good movie',\n",
    "    'I did not like what happened in he end',\n",
    "    'Why woudld the director do this'\n",
    "]\n",
    "\n",
    "batch_2 = vectorization_layer(batch_2)\n",
    "print(batch_2)\n",
    "tf.sqrt(tf.math.count_nonzero(batch_2, axis=-1, keepdims=True, dtype=tf.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:32:38.621154500Z",
     "start_time": "2023-06-26T19:32:38.609030400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\narray([[ 0.00967731, -0.01085351,  0.00827792,  0.01031148,  0.00479365],\n       [-0.00711304,  0.03713636,  0.01846291, -0.0345783 , -0.01964763],\n       [ 0.047338  , -0.13334246,  0.02891386, -0.00703402,  0.05978665]],\n      dtype=float32)>"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_batch = embeddings_layer(batch_2)\n",
    "tf.reduce_sum(embedded_batch, axis=1) / tf.sqrt(tf.math.count_nonzero(batch_2, axis=-1, keepdims=True, dtype=tf.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:34:30.948860100Z",
     "start_time": "2023-06-26T19:34:30.904661500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "class SentenceEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        n_words = tf.math.count_nonzero(input, axis=-1, keepdims=True, dtype=tf.float32)\n",
    "        embeddings_output = self.embedding_layer(input)\n",
    "        return tf.reduce_sum(embeddings_output, axis=1) / tf.sqrt(n_words)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:41:39.587942400Z",
     "start_time": "2023-06-26T19:41:39.538606900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 27s 31ms/step - loss: 0.4347 - binary_accuracy: 0.7974 - val_loss: 0.4102 - val_binary_accuracy: 0.8150\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.3308 - binary_accuracy: 0.8594 - val_loss: 0.3560 - val_binary_accuracy: 0.8436\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 10s 11ms/step - loss: 0.3206 - binary_accuracy: 0.8630 - val_loss: 0.3233 - val_binary_accuracy: 0.8588\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 9s 10ms/step - loss: 0.3134 - binary_accuracy: 0.8648 - val_loss: 0.3630 - val_binary_accuracy: 0.8402\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 8s 9ms/step - loss: 0.3149 - binary_accuracy: 0.8657 - val_loss: 0.3263 - val_binary_accuracy: 0.8581\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fd718a99410>"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_embeddings = tf.keras.models.Sequential([\n",
    "    vectorization_layer,\n",
    "    SentenceEmbedding(input_dim=max_tokens, output_dim=20),\n",
    "])\n",
    "model_4 = tf.keras.models.Sequential([\n",
    "    model_4_embeddings,\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_4.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Nadam(),\n",
    "    metrics=[tf.keras.metrics.binary_accuracy]\n",
    ")\n",
    "model_4.fit(train, epochs=5, validation_data=valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:42:47.841519400Z",
     "start_time": "2023-06-26T19:41:44.099941600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.28287014\n",
      "0.27478057\n",
      "0.2349989\n",
      "0.2859396\n",
      "0.5066771\n"
     ]
    }
   ],
   "source": [
    "queen = model_4_embeddings.predict(['queen'])\n",
    "king = model_4_embeddings.predict(['king'])\n",
    "man = model_4_embeddings.predict(['man'])\n",
    "woman = model_4_embeddings.predict(['woman'])\n",
    "\n",
    "print(distance(king, man))\n",
    "print(distance(king, queen))\n",
    "print(distance(queen, man))\n",
    "print(distance(queen, woman))\n",
    "print(distance(king - man + woman, queen))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:47:43.718279100Z",
     "start_time": "2023-06-26T19:47:43.436105400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "f. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 21:53:16.813489: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/amitaharoni/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4daa5f10b704588bef615195b939efa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98ad50dfc8c84912b1775152bbe9ca19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04854d31ffff439dbc94bfd937f814c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9267f47b61a04625a1d9ba723dde5a64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /home/amitaharoni/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteBGKQ69/imdb_reviews-tr",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea219d05a80c4f14803cbd6af68b03d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b4d346505a94972815b100fd6ee6fe2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /home/amitaharoni/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteBGKQ69/imdb_reviews-te",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f731ec7a764a41879333e1184b3e3de5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a907b56d05e423696af391b65ed37aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /home/amitaharoni/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteBGKQ69/imdb_reviews-un",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2af16b8711674c29a49ed95f866d18f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset imdb_reviews downloaded and prepared to /home/amitaharoni/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:54:08.431549300Z",
     "start_time": "2023-06-26T19:53:16.765761500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 21:54:36.418536: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-06-26 21:54:36.418883: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "2023-06-26 21:54:36.484357: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T19:54:36.555985400Z",
     "start_time": "2023-06-26T19:54:36.411501800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
