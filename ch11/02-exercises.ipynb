{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "What is the problem that Glorot initialization and He initialization aim to fix?\n",
    "> Unstable gradients - Vanishing gradients (typical) or exploding gradients (mostly for RNNs). In vanishing gradients, gradients become smaller and smaller as we progress back to the lower layers. This mean learning in lower layers gets stuck and never converges to a good solution. Saturating activation functions aggravate the problem as their gradients get closer to zero in low or high values. Essentially making the neurons stuck once they get sufficiently large.\n",
    "\n",
    "Correct answer:\n",
    "> Glorot initialization and He initialization were designed to make the output standard deviation as close as possible to the input standard deviation, at least at the beginning of training. This reduces the vanishing/exploding gradients problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "> No, because then it's equivalent to zero (the old problem) if the bias term learns that value. You have to either randomize the weights through a normal distribution or uniform distribution (sqrt(3*sigma^2)) per the initialization formulas.\n",
    "\n",
    "Correct answer:\n",
    "> No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It's like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Is it OK to initialize the bias terms to 0?\n",
    "> Yes. This is in fact what's happening by default."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In which cases would you want to use each of the activation functions we discussed in this chapter?\n",
    "> Sigmoid, tanh - multi-label or binary classification in the output layer\n",
    "> Softmax  multi-class classification in the output layer\n",
    "> ReLU - common activation function in the hidden layers of shallow networks. Could kill neurons.\n",
    "> Leaky ReLU - solve the dead neurons problem with ReLU\n",
    "> Swish - common activation function in the hidden layers of deep networks. But has higher computational cost.\n",
    "> SeLU - self normalizing, i.e. every layer output is normalized with 0 mean and 1 stdev. Only possible in simple MLPs, must standardize inputs, layer must be initialized with LeCun normal, no regularization technique is allowed.\n",
    "\n",
    "> The softplus activation function is useful in the output layer when you need to ensure that the output will always be positive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    " > The sum of past gradients would decay, i.e. the rate of change may explode. Momentum performs an \"exponentially decaying sum\" - not average.\n",
    "\n",
    "Answer:\n",
    "> If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Name three ways you can produce a sparse model.\n",
    "> ReLU to kill neurons\n",
    "> L1 regularization\n",
    "> Dropout  ==> wrong!\n",
    "\n",
    "Correct answer:\n",
    "> One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply â„“1 regularization during training, which pushes the optimizer toward sparsity. A third option is to use the TensorFlow Model Optimization Toolkit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Does dropout slow down training?\n",
    "> It is more likely to speed up training because fewer neurons are being calculated\n",
    "> ==> INCORRECT, actually slows down the convergence process\n",
    "\n",
    "Does it slow down inference (i.e., making predictions on new instances)?\n",
    "> No, as there is no effect on inference - the network architecture is unaffected\n",
    "\n",
    "What about MC dropout?\n",
    "> Slows down inference, as the networks needs to infer K samples per output\n",
    "\n",
    "Correct answer:\n",
    "> Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
